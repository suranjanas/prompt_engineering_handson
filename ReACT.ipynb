{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7557c9fd-c3fa-4764-9ef2-d568674c5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, Tool\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API\"] = os.getenv(\"LANGSMITH_API\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Explore Evaluating index using LLM\" \n",
    "\n",
    "from langchain.agents import Tool \n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    base_url = \"http://localhost:11434\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_function_tools():\n",
    "  search = TavilySearchAPIWrapper()\n",
    "  tavily_tool = TavilySearchResults(api_wrapper=search)\n",
    "\n",
    "  tools = [\n",
    "      tavily_tool\n",
    "  ]\n",
    "\n",
    "  tools.extend(load_tools(['wikipedia']))\n",
    "\n",
    "  return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c617cd-fc93-4a8b-9ac7-44567b70cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suranjanasamanta/opt/anaconda3/envs/promptEngineering/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the age difference, I need to know the birth dates of Amitabh Bachchan and Abhishek Bachchan.\n",
      "Action: wikipedia\n",
      "Action Input: search query for Amitabh Bachchan's birth date\u001b[0m\u001b[33;1m\u001b[1;3mPage: List of films considered the worst\n",
      "Summary: The films listed below have been cited by a variety of notable critics in varying media sources as being among the worst films ever made. Examples of such sources include Metacritic, Roger Ebert's list of most-hated films, The Golden Turkey Awards, Leonard Maltin's Movie Guide, Rotten Tomatoes, pop culture writer Nathan Rabin's My World of Flops, the Stinkers Bad Movie Awards, the cult TV series Mystery Science Theater 3000 (alongside spinoffs Cinematic Titanic, The Film Crew and RiffTrax), and the Golden Raspberry Awards (aka the \"Razzies\"). Films on these lists are generally feature-length films that are commercial/artistic in nature (intended to turn a profit, express personal statements or both), professionally or independently produced (as opposed to amateur productions, such as home movies), and released in theaters, then on home video.\u001b[0m"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "tools = get_function_tools()\n",
    "agent = create_react_agent(llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, verbose=True)\n",
    "\n",
    "question = \"\"\"Who is the greatest astronaut?\"\"\"\n",
    "question = \"\"\"Roger has 3 tennis balls. He bought 2 more cans of tennis balls. Each can ontains 5 tennis balls. How many tennis balls does Roger has?\"\"\"\n",
    "question = \"\"\"What is the age difference between Amitabh Bachchan and Abhishek Bachchan?\"\"\"\n",
    "\n",
    "output = agent_executor.invoke({\"input\": question})\n",
    "process_chunks(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a271d-da04-4dcf-96a4-41a69329cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from rich.console import Console\n",
    "\n",
    "# Initialize dotenv to load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Rich for better output formatting and visualization\n",
    "rich = Console()\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "\n",
    "# Initialize Tavily\n",
    "tavily = TavilySearchResults(max_results=3)\n",
    "\n",
    "# Create a LangGraph agent\n",
    "langgraph_agent = create_react_agent(model=llm, tools=[tavily])\n",
    "\n",
    "\n",
    "# Define a function to process chunks from the agent\n",
    "def process_chunks(chunk):\n",
    "    \"\"\"\n",
    "    Process a chunk of data and extract information about tool calls made by the agent.\n",
    "\n",
    "    Parameters:\n",
    "        chunk (dict): A dictionary containing information about the agent's messages.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This function processes a chunk of data and checks if it contains information about an agent. If the chunk contains\n",
    "    an agent's message, it iterates over the messages in the agent's messages. For each message, it checks if the\n",
    "    message contains tool calls. If a tool call is found, the function extracts the tool name and query from the\n",
    "    message and prints a formatted message using the Rich library. If no tool call is found, the function extracts\n",
    "    the agent's answer from the message and prints it using the Rich library.\n",
    "\n",
    "    The function uses the Rich library for formatting and printing the messages.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the chunk contains an agent's message\n",
    "    if \"agent\" in chunk:\n",
    "        # Iterate over the messages in the chunk\n",
    "        for message in chunk[\"agent\"][\"messages\"]:\n",
    "            # Check if the message contains tool calls\n",
    "            if \"tool_calls\" in message.additional_kwargs:\n",
    "                # If the message contains tool calls, extract and display an informative message with tool call details\n",
    "\n",
    "                # Extract all the tool calls\n",
    "                tool_calls = message.additional_kwargs[\"tool_calls\"]\n",
    "\n",
    "                # Iterate over the tool calls\n",
    "                for tool_call in tool_calls:\n",
    "                    # Extract the tool name\n",
    "                    tool_name = tool_call[\"function\"][\"name\"]\n",
    "\n",
    "                    # Extract the tool query\n",
    "                    tool_arguments = eval(tool_call[\"function\"][\"arguments\"])\n",
    "                    tool_query = tool_arguments[\"query\"]\n",
    "\n",
    "                    # Display an informative message with tool call details\n",
    "                    rich.print(\n",
    "                        f\"\\nThe agent is calling the tool [on deep_sky_blue1]{tool_name}[/on deep_sky_blue1] with the query [on deep_sky_blue1]{tool_query}[/on deep_sky_blue1]. Please wait for the agent's answer[deep_sky_blue1]...[/deep_sky_blue1]\",\n",
    "                        style=\"deep_sky_blue1\",\n",
    "                    )\n",
    "            else:\n",
    "                # If the message doesn't contain tool calls, extract and display the agent's answer\n",
    "\n",
    "                # Extract the agent's answer\n",
    "                agent_anser = message.content\n",
    "\n",
    "                # Display the agent's answer\n",
    "                rich.print(f\"\\nAgent:\\n{agent_anser}\", style=\"black on white\")\n",
    "\n",
    "\n",
    "# Loop until the user chooses to quit the chat\n",
    "while True:\n",
    "    # Get the user's question and display it in the terminal\n",
    "    user_question = input(\"\\nUser:\\n\")\n",
    "\n",
    "    # Check if the user wants to quit the chat\n",
    "    if user_question.lower() == \"quit\":\n",
    "        rich.print(\"\\nAgent:\\nHave a nice day! :wave:\\n\", style=\"black on white\")\n",
    "        break\n",
    "\n",
    "    # Use the stream method of the LangGraph agent to get the agent's answer\n",
    "    for chunk in langgraph_agent.stream(\n",
    "        {\"messages\": [HumanMessage(content=user_question)]}\n",
    "    ):\n",
    "        # Process the chunks from the agent\n",
    "        process_chunks(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed679c-887d-45f0-8844-f077421cbe47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
